{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fifty-arctic",
   "metadata": {},
   "source": [
    "Name - Ruoqing(Sherry) Xie\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-progress",
   "metadata": {},
   "source": [
    "Research Question / Hypothesis:\n",
    "----\n",
    "### Loan-approval prediction - target: predict which loan should not be approval by the loan payoff data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "individual-frost",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in /Users/lolu/opt/anaconda3/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: boto3 in /Users/lolu/opt/anaconda3/lib/python3.8/site-packages (1.17.23)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /Users/lolu/opt/anaconda3/lib/python3.8/site-packages (from boto3) (0.3.4)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/lolu/opt/anaconda3/lib/python3.8/site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.23 in /Users/lolu/opt/anaconda3/lib/python3.8/site-packages (from boto3) (1.20.23)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/lolu/opt/anaconda3/lib/python3.8/site-packages (from botocore<1.21.0,>=1.20.23->boto3) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /Users/lolu/opt/anaconda3/lib/python3.8/site-packages (from botocore<1.21.0,>=1.20.23->boto3) (1.26.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/lolu/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.23->boto3) (1.15.0)\n",
      "Requirement already satisfied: imbalanced-learn in /Users/lolu/opt/anaconda3/lib/python3.8/site-packages (from imblearn) (0.8.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/lolu/opt/anaconda3/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/lolu/opt/anaconda3/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.19.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/lolu/opt/anaconda3/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.5.2)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in /Users/lolu/opt/anaconda3/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (0.24.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/lolu/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.24->imbalanced-learn->imblearn) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install imblearn boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "informal-danish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /Users/lolu/opt/anaconda3/lib/python3.8/site-packages (1.3.1)\n",
      "Requirement already satisfied: numpy in /Users/lolu/opt/anaconda3/lib/python3.8/site-packages (from xgboost) (1.19.2)\n",
      "Requirement already satisfied: scipy in /Users/lolu/opt/anaconda3/lib/python3.8/site-packages (from xgboost) (1.5.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "manual-difference",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "widespread-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing  import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import *\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from xgboost import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ordered-acceptance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score, make_scorer\n",
    "from sklearn.base    import BaseEstimator\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "surgical-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-medicaid",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "quarterly-appreciation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from s3 - local access\n",
    "bucket = \"ruoqingxie\"\n",
    "file_name1 = \"loan_data/SBAnational.csv\"\n",
    "file_name2 = \"loan_data/gdp.csv\"\n",
    "\n",
    "s3 = boto3.client('s3') \n",
    "\n",
    "obj1 = s3.get_object(Bucket= bucket, Key= file_name1)\n",
    "obj2 = s3.get_object(Bucket= bucket, Key= file_name2)\n",
    "\n",
    "loan = pd.read_csv(obj1['Body']) # 'Body' is a key word\n",
    "gdp = pd.read_csv(obj2['Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "working-light",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Found some misssing value in target variables, \n",
    "# it's not helpful in the model and the missing row is less that 1% of the data, so drop them.\n",
    "loan = loan[loan['MIS_Status'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "promotional-program",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(897167, 27)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "developed-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target: loan defaul or not\n",
    "# convert the loan from object to number: 1- default 0 - Not default\n",
    "# could use lable encoding but applymap works as well\n",
    "target = loan[['MIS_Status']].applymap(lambda x: 1 if x == \"CHGOFF\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "decreased-removal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only need column gdp growth\n",
    "gdp_dict = gdp.set_index('Year').to_dict()['GDPGrowth']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-royalty",
   "metadata": {},
   "source": [
    "## 2. Data processing\n",
    "### 2.1 Cleaning exiting features\n",
    "1. resorted bank according to frequency appears in dataset instead of using bank name\n",
    "2. resorted city according to frequency appears in dataset instead of using city name\n",
    "2. drop data leakge columns and helpless columns\n",
    "3. clean money, get rid of dollar sign\n",
    "4. extract month and FY year only from date\n",
    "5. convert NewExist to 0 and 1\n",
    "6. only keep NAICS first 2 digit(it represents industry) and convert it to categorical data\n",
    "7. convert data type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-elements",
   "metadata": {},
   "source": [
    "1. resorted bank according to frequency appears in dataset instead of using bank name:\\\n",
    "I noticed there is 5802 unique bank names in the data set, in some similar research, bank name was drop because they consider name as an irrelative variable when decided if a loan would be default. However, I would consider the bank name might help. Large bank would tend to have a higher standard to review the borrowerâ€™s information, so they might have a lower default rate, and small bank might have a lower bar for borrowers. Therefore, instead if using the bank name, I decide to resort the bank by the frequency they appear in dataset. The higher frequency indicated a larger scale of the bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "finished-plenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_bank = list(loan['Bank'].value_counts()[loan['Bank'].value_counts().values >= 1000].keys())\n",
    "med_bank = list(loan['Bank'].value_counts()[(loan['Bank'].value_counts().values < 1000)]\\\n",
    "[loan['Bank'].value_counts()\\\n",
    "[(loan['Bank'].value_counts().values < 1000)]>10].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "reflected-concrete",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_city = list(loan['City'].value_counts()[loan['City'].value_counts().values > 1000].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "environmental-arrest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_sorted_bank(X):\n",
    "    out = ['other'] * len(X)\n",
    "    for i,name in enumerate(X['Bank'].values):\n",
    "        if name in large_bank:\n",
    "            out[i] = 'large'\n",
    "        if name in med_bank:\n",
    "            out[i] = 'med'\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    X['Bank'] = out\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-playlist",
   "metadata": {},
   "source": [
    "2. resorted city according to frequency appears in dataset instead of using city name\\\n",
    "Same reasons for city name. The name itself might not affect the loan default rate, but the scale and population of a city where borrower live might affect. So I resort the city names by frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "southwest-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resorted_city(x):\n",
    "    out = ['other'] * len(x)\n",
    "    for i, city in enumerate(x['City'].values):\n",
    "        if city in large_city:\n",
    "            out[i] = 'large'\n",
    "        else:\n",
    "            continue\n",
    "    x['City'] = out\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-penny",
   "metadata": {},
   "source": [
    "3. drop data leakge columns and helpless columns\\\n",
    "Since the purpose of my model is to help the bank to identify if a loan would default at the beginning stage, there are some information we are not able to acquire:\n",
    "- ChgOffDate: date to payoff the loan\n",
    "- ChgOffPrinGr: the chage off amount at the payoff date\n",
    "- BalanceGross: gross amount not pay at charge of date\n",
    "- Mis_status: the target variable, must drop.\n",
    "\n",
    "Also, there are some information I don't think it would be helpful for the prediction:\n",
    "- Name: borrower's name\n",
    "- LoanNr_ChkDgt: loan number\n",
    "- zip: zip code is not useful since we already have city and state\n",
    "- FranchiseCode: a primary id code.\n",
    "- DisbursementDate: data that the loan was distributed(it largly depends on the bank's efficiency)\n",
    "- BankState: we already have \"state\".\n",
    "\n",
    "Last for the columns with more than 30% missing values and with less important infomation I decide to drop it:\n",
    "- LowDoc: weather a loan document can be written as one page.\n",
    "- RevLineCr: weather Revolving credit is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "driven-subscription",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_cols(X):\n",
    "    \"\"\"drop data leckage and helpless column\"\"\"\n",
    "    X = X.drop(columns = ['ChgOffDate', 'RevLineCr','LowDoc','DisbursementDate', \n",
    "                          'MIS_Status', 'Name', 'FranchiseCode', 'BankState',\n",
    "                          'LoanNr_ChkDgt', 'Zip', 'BalanceGross', \n",
    "                          'ChgOffPrinGr', 'DisbursementDate'])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-pharmacy",
   "metadata": {},
   "source": [
    "4. clean money, get rid of dollar sign\\\n",
    "The dollar amount from the dataset are identified as \"object\" type because of the dollar sign, so I take off the dollar sign and convert the datatype to \"float\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "antique-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \"$\" and convert to float\n",
    "def money_clean(x):\n",
    "    out = x.replace('$', '').replace(',','').strip()\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-equity",
   "metadata": {},
   "source": [
    "5. extract month from date\\\n",
    "keeping the whole date is not helpful to identify the dafualt laon, since we already have a column list Fiscal year, the most useful information we can get from date is the month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "retained-storage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only want month of approval\n",
    "def date_clean(x):\n",
    "    out = x.strip().split('-')[1]\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dynamic-arnold",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap money clean and date clean info the same function\n",
    "def sign_clean(x):\n",
    "    col_money = ['DisbursementGross', 'GrAppv', 'SBA_Appv']\n",
    "    x[col_money] = x[col_money].applymap(money_clean)\n",
    "    x['ApprovalDate'] = x[['ApprovalDate']].applymap(date_clean)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "whole-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the typo in FY and convert the data type to int\n",
    "def clean_FY(x):\n",
    "    x['ApprovalFY'] = x[['ApprovalFY']].applymap(lambda y: int(y.strip().replace('A','')) \n",
    "                                                   if type(y) == str else int(y))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-surveillance",
   "metadata": {},
   "source": [
    "6. convert NewExist to 0 and 1\\\n",
    "The original dataset use 1 and 2 to represent yes and no, however, it makes more sense to use 1 and 0 in a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "caring-blues",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_newExt(x):\n",
    "    x['NewExist'] = x[['NewExist']].applymap(lambda x: 0 if x == 1.0 else 1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-assembly",
   "metadata": {},
   "source": [
    "7. only keep NAICS first 2 digit(it represents industry) and convert it to categorical data\\\n",
    "The first 2 digits in NAICS represent the industry the borrower's companies belong to. So we extract the frist 2 digit and convert it as a categorical variable so that we can use it to represent \"industry\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "innocent-alloy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_NAICS(x):\n",
    "    x['NAICS'] = x[['NAICS']].applymap(lambda x: str(x)[:2] if x != 0 else '0')\n",
    "    x['NAICS'] = x[['NAICS']].applymap(lambda x: x.replace('32', '31').replace('33', '31')\\\n",
    "                                                  .replace('45', '44'). replace('49','48'))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "czech-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data type for moeny\n",
    "def type_convert(x):\n",
    "    x = x.astype({'DisbursementGross': 'float64', \n",
    "                   'GrAppv': 'float64', 'SBA_Appv': 'float64'})\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-logistics",
   "metadata": {},
   "source": [
    "### 2.2 Adding new feature\n",
    "Here I added 3 new features to help us identify the default loan:\n",
    "1. Loans Backed by Real Estate( term > 240)\\\n",
    "The first thing I consider is if this loan is a martage-back loan. A mortaged-back loan would be more secure. According to the law, all the loan with a term > 240 months must be a mortaged-back loan, so I create a feature call RealEst and return 1 if the loan is back by a real estste proprety.\n",
    "2. GDP grow rate instead of FY\\\n",
    "The second thing I notice from EDA(Not shown here) is that the default rate greatly increase during 2008 and 1970s. However, the year number itself are not connected to economic environment. So I decide to add one addtional feature call GDPGrowth measuring the current year's gpd growth rate comparing to the last year. This feature would help to indentify the economics environment is getting better or worse.\n",
    "3. SBA-guaranteed percentage\n",
    "we consider a higher percantage approve by SBA would indicate the loan is a more secure loan. Adding this feature help to specify the realtion between SMB approval amount and bank approval amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "framed-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creatRealEst(x):\n",
    "    x['RealEst'] = [0 if i< 240 else 1 for i in x['Term']]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "diagnostic-trail",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creatSMBpct(x):\n",
    "    x['SMBApprovelRate'] = x['SBA_Appv']/x['GrAppv']\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "minimal-nothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creatGDPGrowth(x):\n",
    "    x['GDPGrowth'] = [gdp_dict[i] for i in x['ApprovalFY'].values]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-grain",
   "metadata": {},
   "source": [
    "### 2.3 Wrap all feature transform in one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "respective-measure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_proessing(x):\n",
    "    x = drop_cols(x)\n",
    "    x = re_sorted_bank(x)\n",
    "    x = resorted_city(x)\n",
    "    x = sign_clean(x)\n",
    "    x = clean_newExt(clean_FY(x))\n",
    "    x = clean_NAICS(x)\n",
    "    x = type_convert(x)\n",
    "    x = creatRealEst(x)\n",
    "    x = creatSMBpct(x)\n",
    "    x = creatGDPGrowth(x)\n",
    "    return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "baking-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_transformer = FunctionTransformer(data_proessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "binary-awareness",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['City', 'State', 'Bank', 'NAICS', 'ApprovalDate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "sixth-agenda",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_columns = ['ApprovalFY', 'Term', 'NoEmp', 'NewExist', 'CreateJob', 'RetainedJob', 'UrbanRural',\n",
    "               'DisbursementGross', 'GrAppv', 'SBA_Appv', 'RealEst', 'SMBApprovelRate','GDPGrowth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "smooth-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = target['MIS_Status'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "conceptual-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = loan.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "lucky-contractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-battle",
   "metadata": {},
   "source": [
    "## 3. Pipeline building & parameter tuning\n",
    "----\n",
    "Use 3 models to measure:\n",
    "1. Random forest calssification\n",
    "2. Logistic Regression\n",
    "3. Xgboosting classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "accessory-rocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyEstimator(BaseEstimator):\n",
    "    \"Pass through class, methods are present but do nothing.\"\n",
    "    def fit(self): pass\n",
    "    def score(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "detected-indiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for converting/processing continuous data\n",
    "# We don't have too much missing values here, so use most frequent data to replace them\n",
    "con_pipe = Pipeline([('impute',       SimpleImputer(strategy=\"most_frequent\")),\n",
    "                     ('scaler', StandardScaler())])\n",
    "\n",
    "# Pipeline for converting/processing categorical data\n",
    "cat_pipe = Pipeline([('impute',       SimpleImputer(strategy=\"most_frequent\")),\n",
    "                     ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessing = ColumnTransformer([('categorical', cat_pipe, cat_columns),\n",
    "                                   ('continuous', con_pipe, con_columns)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-catch",
   "metadata": {},
   "source": [
    "### Using RandomizedSearchCV among 3 models and get the best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-jungle",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_sc = Pipeline([('feature', feature_transformer),\n",
    "                    ('preprocessing', preprocessing),\n",
    "                    ('clf', DummyEstimator())])\n",
    "\n",
    "# do a RandomizedSearchCV search among the given 3 models, find the best model and best paramater\n",
    "search_space = [{'clf': [LogisticRegression()], # LogisticRegression\n",
    "                 'clf__penalty': ['l1', 'l2'], # specify the norm used in the penalization in regulazation\n",
    "                 'clf__C': np.logspace(0, 5)}, # explore the effect of regularization \n",
    "                \n",
    "                {'clf': [RandomForestClassifier()],  # RandomForest\n",
    "                 'clf__criterion': ['gini', 'entropy'], # try effect of different loss function\n",
    "                 'clf__max_depth': [3, 5], # try different depth for tree, contol the number of tree layers\n",
    "                 'clf__min_samples_leaf': [1, 3], # control min sample number each tree takes\n",
    "                 'clf__n_estimators': [100, 200]}, # control max trees number generate by model\n",
    "                \n",
    "                {'clf': [XGBClassifier()],  # xgboosting\n",
    "                 # try the loss function to calculate festure importance\n",
    "                 'clf__importance_type': [\"gain\", \"weight\", \"total_cover\"],\n",
    "                 'clf__max_depth': [3, 6], # contol the number of tree depth, prevent overfitting\n",
    "                 'clf__learning_rate': [0.2, 0.3], # control the step of gradient decent to find the optimal\n",
    "                 'clf__n_estimators': [100, 200]}] # control max trees number generate by model\n",
    "\n",
    "# Good ol' random search\n",
    "clf_algos_rand = RandomizedSearchCV(estimator=pipe_sc, \n",
    "                                    param_distributions=search_space, \n",
    "                                    n_iter=25,\n",
    "                                    # use balanced_acc because we are positive and negative evenly\n",
    "                                    scoring = 'balanced_accuracy', \n",
    "                                    cv=5, # 5-fold cross validation to get a genralized result\n",
    "                                    n_jobs=-1)\n",
    "\n",
    "best_model = clf_algos_rand.fit(X_train, y_train)\n",
    "\n",
    "# get the paramenter for best model\n",
    "best_model.best_estimator_.get_params()['clf'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-absolute",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-cycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_xgb = Pipeline([('feature', feature_transformer),\n",
    "                     ('preprocessing', preprocessing),\n",
    "                     ('best_model', best_model.best_estimator_.get_params()['clf'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-practice",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-popularity",
   "metadata": {},
   "source": [
    "## 4. Evaluation Metric\n",
    "----\n",
    "\n",
    "The data is a binomial imbalanced class, and here, I would like to put evenly weight on both posotove and negative classification, so we use **balanced_accuracy_score** here as a baseline metric, it simply measures generally what the probability that model would put the loan to a correct class. We want balanced weight on both postive class and negative class because the purpose of bank activities should be maximizing the profit by controling the risk instead of only considering the profit.\n",
    "\n",
    "Also, print out the confustion matrix to check **Specificity**: $\\frac{True negative}{conditional negative}$, because I would like to see given a loan is default, what is the probability that the model would make a correct classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-lobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb = pipe_xgb.predict(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-swiss",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The balanced prediction accuracy is {balanced_accuracy_score(y_validation, y_pred_xgb):.4f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-optics",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm = confusion_matrix(y_validation, y_pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Specificity is {cfm[1][1]/(cfm[0][1]+cfm[1][1]):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-realtor",
   "metadata": {},
   "source": [
    "## 5. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-capital",
   "metadata": {},
   "source": [
    "### 5.1 The best fit model with best fit parameter - xgboosting\n",
    "```python\n",
    "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
    "              importance_type='total_cover', interaction_constraints='',\n",
    "              learning_rate=0.2, max_delta_step=0, max_depth=6,\n",
    "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
    "              n_estimators=200, n_jobs=16, num_parallel_tree=1, random_state=0,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
    "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-gravity",
   "metadata": {},
   "source": [
    "### 5.2 All pipelines steps and all non-default hyperparameters\n",
    "``` python\n",
    "Pipeline(steps=[('feature',\n",
    "                 FunctionTransformer(func=<function data_proessing at 0x7ff9c5fb28b0>)),\n",
    "                ('preprocessing',\n",
    "                 ColumnTransformer(transformers=[('categorical',\n",
    "                                                  Pipeline(steps=[('impute',\n",
    "                                                                   SimpleImputer(strategy='most_frequent')),\n",
    "                                                                  ('ohe',\n",
    "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
    "                                                  ['City', 'State', 'Bank',\n",
    "                                                   'NAICS', 'ApprovalDate']),\n",
    "                                                 ('continuous',\n",
    "                                                  Pipeline(step...\n",
    "                               colsample_bytree=1, gamma=0, gpu_id=-1,\n",
    "                               importance_type='total_cover',\n",
    "                               interaction_constraints='', learning_rate=0.2,\n",
    "                               max_delta_step=0, max_depth=6,\n",
    "                               min_child_weight=1, missing=nan,\n",
    "                               monotone_constraints='()', n_estimators=200,\n",
    "                               n_jobs=16, num_parallel_tree=1, random_state=0,\n",
    "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
    "                               subsample=1, tree_method='exact',\n",
    "                               validate_parameters=1, verbosity=None))])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-guitar",
   "metadata": {},
   "source": [
    "### 5.3 Result interpretation\n",
    "\n",
    "By doing a random CV search, I found out xgboosting is the best model in perdiction. Xgboosting is a tree-base algorithm using gradient boosting as ensemble method. Bossting is a dynamic ensemble method which updated the tree weight according to the previous step's loss and classification result. Gradient Boosting use Gradient Descent to minimize the loss function in boosting ensemble.\n",
    "\n",
    "The result from xgb model show below:\n",
    "\n",
    "|          |  Not default  | default|\n",
    "|----------|:-------------:|:------:|\n",
    "|Not default (Actual)|    576085   |  15565  |\n",
    "|default (Actual) |     20890    |    105194 |\n",
    "\n",
    "The model successfully calssify 87% of default loan. In other words, given an upcoming loan's information, the model has 90% probabilty to tell if this loan would default correctly. And given a loan is default loan, the model has 87%  probabilty to classify it correctly.\n",
    "\n",
    "The model can greatly help a bank to decide weather a loan should be approved or not by simplying getting all related input data into this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-offer",
   "metadata": {},
   "source": [
    "### 5.4. Further to do\n",
    "\n",
    "1. Find the best split for bank and city. The split in frequency would affect the classification result, if I have more time, I would select the split more carefully by applying gini loss.\n",
    "2. Find better measurement for economy environment. GDP growth rate is the most obvious measurement for economy, however, it might be too marco. If we would like to track the economy correctly, some other factors like unemployment rate, national debt rate and bank long run interest rate can also be a good measurement.\n",
    "3. It's an imbalanced dataset, in this notebook I use balanced accurancy and balanced weighted to reduce the effect of imbalanced. Since the whole dataset is large, upsampling is not applied. However, further researh can pay attention to upsampling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
